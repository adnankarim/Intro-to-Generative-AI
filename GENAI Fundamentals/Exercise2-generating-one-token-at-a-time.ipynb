{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Generating one token at a time\n",
    "\n",
    "In this exercise, we will get to understand how an LLM generates text--one token at a time, using the previous tokens to predict the following ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load a tokenizer and a model\n",
    "\n",
    "First we load a tokenizer and a model from HuggingFace's transformers library. A tokenizer is a function that splits a string into a list of numbers that the model can understand.\n",
    "\n",
    "In this exercise, all the code will be written for you. All you need to do is follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of NLP and Hugging Face's `transformers` library, **tokens** are the basic units into which a piece of text is split before being fed into a model. The model processes these tokens as numerical representations.\n",
    "\n",
    "Hereâ€™s how **tokens** are defined and used in these cases:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What are tokens?**\n",
    "- Tokens are **smaller units of text**, such as:\n",
    "  - Words: \"Udacity\", \"is\", \"the\", etc.\n",
    "  - Subwords: Complex words like \"generative\" may be split into \"gener\", \"ative\".\n",
    "  - Special characters: Punctuation marks like `.` or `,` are also treated as tokens.\n",
    "  - Spaces: Some tokenizers may treat spaces as tokens.\n",
    "  \n",
    "These tokens are mapped to **unique numerical IDs** using a tokenizer's vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How does tokenization work in GPT-2?**\n",
    "GPT-2 uses a **Byte Pair Encoding (BPE)** tokenizer, which splits text into subwords to balance vocabulary size and coverage. Here's what happens during tokenization:\n",
    "\n",
    "- **Word-level tokens**: Simple words like \"Udacity\" or \"is\" are tokens.\n",
    "- **Subword tokens**: For words not in the vocabulary (e.g., \"generative\"), it may split into smaller subunits, such as \"gener\" and \"ative\".\n",
    "- **Character-level tokens**: Rare or unusual characters may be treated as standalone tokens.\n",
    "- **Special tokens**: Reserved tokens such as `<|endoftext|>` represent special functions like indicating the end of a sentence.\n",
    "\n",
    "For example:\n",
    "- Input: `\"Udacity is the best place to learn about generative\"`\n",
    "- Tokenized:\n",
    "  - `\"Udacity\"` â†’ Token ID 7597\n",
    "  - `\"is\"` â†’ Token ID 318\n",
    "  - `\"the\"` â†’ Token ID 262\n",
    "  - `\"best\"` â†’ Token ID 674\n",
    "  - `\"place\"` â†’ Token ID 1476\n",
    "  - `\"to\"` â†’ Token ID 284\n",
    "  - `\"learn\"` â†’ Token ID 1005\n",
    "  - `\"about\"` â†’ Token ID 833\n",
    "  - `\"generative\"` â†’ Tokenized into subwords, e.g., `\"gener\"` â†’ 12345, `\"ative\"` â†’ 67890.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Why are tokens important?**\n",
    "Tokens are critical for:\n",
    "1. **Model Understanding**: Neural networks process numbers, not text. Token IDs convert text into a format the model understands.\n",
    "2. **Efficiency**: Subword tokenization (like in GPT-2) ensures that:\n",
    "   - Rare words are split into reusable subunits.\n",
    "   - The model doesnâ€™t need to store an enormous vocabulary.\n",
    "3. **Flexibility**: It allows handling of unseen words or complex languages.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What do tokens look like in this code?**\n",
    "When you inspect `inputs[\"input_ids\"]`, you see a tensor of numerical IDs, representing the sequence of tokens. For example:\n",
    "```python\n",
    "inputs[\"input_ids\"] = tensor([[7597, 318, 262, 674, 1476, 284, 1005, 833, 12345, 67890]])\n",
    "```\n",
    "This is the tokenized representation of `\"Udacity is the best place to learn about generative\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Key Advantages of Tokenization**\n",
    "- **Compact Vocabulary**: Subword tokenization minimizes the size of the vocabulary while still covering diverse text.\n",
    "- **Handling Rare Words**: Instead of treating \"generative\" as one token, splitting it into \"gener\" and \"ative\" allows the model to handle similar words (e.g., \"generation\") efficiently.\n",
    "- **Preprocessing for Models**: Converts human-readable text into numerical input.\n",
    "\n",
    "You can explore the tokens more explicitly using:\n",
    "```python\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "```\n",
    "This will give you the actual tokens corresponding to the numerical IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342ca852ba234f3ebff51d4bd26f0fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f9ebb897d74f36b18388b59de49900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94747ccb54d49499947dd0e49c0dfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f8a9e7671a4d6e8e7ded5fbeeb49f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2991f692078049dd8858cfadfbac485d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e64e547edb443094bebbd626330a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e87f92575f4c958da4935ab1fd4778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# To load a pretrained model and a tokenizer using HuggingFace, we only need two lines of code!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# We create a partial sentence and tokenize it.\n",
    "text = \"Udacity is the best place to learn about generative\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Show the tokens as numbers, i.e. \"input_ids\"\n",
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Examine the tokenization\n",
    "\n",
    "Let's explore what these tokens mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(52)</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(67)</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(4355)</td>\n",
       "      <td>acity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(318)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(262)</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(1266)</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tensor(1295)</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensor(284)</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tensor(2193)</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tensor(546)</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tensor(1152)</td>\n",
       "      <td>gener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tensor(876)</td>\n",
       "      <td>ative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id   token\n",
       "0     tensor(52)       U\n",
       "1     tensor(67)       d\n",
       "2   tensor(4355)   acity\n",
       "3    tensor(318)      is\n",
       "4    tensor(262)     the\n",
       "5   tensor(1266)    best\n",
       "6   tensor(1295)   place\n",
       "7    tensor(284)      to\n",
       "8   tensor(2193)   learn\n",
       "9    tensor(546)   about\n",
       "10  tensor(1152)   gener\n",
       "11   tensor(876)   ative"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how the sentence is tokenized\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def show_tokenization(inputs):\n",
    "    return pd.DataFrame(\n",
    "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
    "        columns=[\"id\", \"token\"],\n",
    "    )\n",
    "\n",
    "\n",
    "show_tokenization(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(52), 'U'),\n",
       " (tensor(67), 'd'),\n",
       " (tensor(4355), 'acity'),\n",
       " (tensor(318), ' is'),\n",
       " (tensor(262), ' the'),\n",
       " (tensor(1266), ' best'),\n",
       " (tensor(1295), ' place'),\n",
       " (tensor(284), ' to'),\n",
       " (tensor(2193), ' learn'),\n",
       " (tensor(546), ' about'),\n",
       " (tensor(1152), ' gener'),\n",
       " (tensor(876), 'ative')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "\n",
    "The interesting thing is that tokens in this case are neither just letters nor just words. Sometimes shorter words are represented by a single token, but other times a single token represents a part of a word, or even a single letter. This is called subword tokenization.\n",
    "\n",
    "## Step 2. Calculate the probability of the next token\n",
    "\n",
    "Now let's use PyTorch to calculate the probability of the next token given the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>.</td>\n",
       "      <td>0.352222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>,</td>\n",
       "      <td>0.135989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>and</td>\n",
       "      <td>0.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>in</td>\n",
       "      <td>0.069530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8950</th>\n",
       "      <td>8950</td>\n",
       "      <td>languages</td>\n",
       "      <td>0.058291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       token         p\n",
       "13      13           .  0.352222\n",
       "11      11           ,  0.135989\n",
       "290    290         and  0.109372\n",
       "287    287          in  0.069530\n",
       "8950  8950   languages  0.058291"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the probabilities for the next token for all possible choices. We show the\n",
    "# top 5 choices and the corresponding words or subwords for these tokens.\n",
    "\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "\n",
    "def show_next_token_choices(probabilities, top_n=5):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (id, tokenizer.decode(id), p.item())\n",
    "            for id, p in enumerate(probabilities)\n",
    "            if p.item()\n",
    "        ],\n",
    "        columns=[\"id\", \"token\", \"p\"],\n",
    "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
    "\n",
    "\n",
    "show_next_token_choices(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-111.5971])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.logits[:,-1,90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! The model thinks that the most likely next word is \"programming\", followed up closely by \"learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token id: 301826\n",
      "Next token: \n"
     ]
    }
   ],
   "source": [
    "# Obtain the token id for the most probable next token\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"Next token id: {next_token_id}\")\n",
    "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Udacity is the best place to learn about generative programming. programming'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We append the most likely token to the text.\n",
    "text = text + tokenizer.decode(8300)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Generate some more tokens\n",
    "\n",
    "The following cell will take `text`, show the most probable tokens to follow, and append the most likely token to text. Run the cell over and over to see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Udacity is the best place to learn about generative programming. programming\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Next token probabilities:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>is</td>\n",
       "      <td>0.125294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8950</th>\n",
       "      <td>8950</td>\n",
       "      <td>languages</td>\n",
       "      <td>0.078196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>.</td>\n",
       "      <td>0.062432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>in</td>\n",
       "      <td>0.051712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>351</td>\n",
       "      <td>with</td>\n",
       "      <td>0.032723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       token         p\n",
       "318    318          is  0.125294\n",
       "8950  8950   languages  0.078196\n",
       "13      13           .  0.062432\n",
       "287    287          in  0.051712\n",
       "351    351        with  0.032723"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Press ctrl + enter to run this cell again and again to see how the text is generated.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Show the text\n",
    "print(text)\n",
    "\n",
    "# Convert to tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Calculate the probabilities for the next token and show the top 5 choices\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "display(Markdown(\"**Next token probabilities:**\"))\n",
    "display(show_next_token_choices(probabilities))\n",
    "\n",
    "# Choose the most likely token id and add it to the text\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "text = text + tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use the `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Once upon a time, generative models of the human brain were used to study the neural correlates of cognitive function. In the present study, we used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Start with some text and tokenize it\n",
    "text = \"Once upon a time, generative models\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Use the `generate` method to generate lots of text\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Show the generated text\n",
    "display(Markdown(tokenizer.decode(output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's interesting...\n",
    "\n",
    "You'll notice that GPT-2 is not nearly as sophisticated as later models like GPT-4, which you may have experience using. It often repeats itself and doesn't always make much sense. But it's still pretty impressive that it can generate text that looks like English.\n",
    "\n",
    "## Congrats for completing the exercise! ðŸŽ‰\n",
    "\n",
    "Give yourself a hand. And please take a break if you need to. We'll be here when you're refreshed and ready to learn more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
